import boto.ec2
import collections
import json
import math
import os
import paramiko
import random
import socket
import subprocess
import tempfile
import time

import platform


def ssh_just_run(client, cmd):
    ch = client.get_transport().open_session()
    ch.exec_command(cmd)
    ch.recv_exit_status()


class BasicEC2Platform(platform.Platform):

    name = 'Runs tasks over a given number of instances.'

    def __init__(self):
        super(BasicEC2Platform, self).__init__()

    def args(self, parser):
        parser.add_argument('id', type=str, help='AWS "access key id"')
        parser.add_argument('secret', type=str, help='AWS "secret access key"')
        parser.add_argument('key_name', type=str)
        parser.add_argument('region', type=str, help='Which region')
        parser.add_argument('ami', type=str, help='Which amazon machine image')
        parser.add_argument('instance_type', type=str, help='What type of instance')
        parser.add_argument('instance_count', type=int, help='How many instances')
        self._args = parser.parse_args()

    def execute(self):

        # Divide up tasks among workers.
        tasks = list(self._tasks)
        random.shuffle(tasks)
        queues = dict((i, []) for i in range(min(self._args.instance_count, len(self._tasks))))
        evenly_count = len(tasks) / self._args.instance_count
        for q in queues:
            queues[q].extend(tasks[:evenly_count])
            tasks = tasks[evenly_count:]
        for i in range(len(tasks) % self._args.instance_count):
            queues[queues.keys()[i]].append(tasks.pop())

        # Make a sources tarball.
        fd, source_tarball_fn = tempfile.mkstemp('.tar.gz')
        os.close(fd)
        subprocess.check_call('tar czf %s $(git ls-files)' % source_tarball_fn, shell=True)

        # Connect.
        conn = boto.ec2.connect_to_region(self._args.region, aws_access_key_id=self._args.id, aws_secret_access_key=self._args.secret)
        assert conn is not None

        # See if the '--ports=${port1,port2,..}' securitygroup exists,
        # or make it.
        ports = [22]
        secgroup_name = '--tcp=%s' % ','.join(map(str, ports))
        if all(map(lambda g: g.name != secgroup_name, conn.get_all_security_groups())):
            g = conn.create_security_group(secgroup_name, 'Generated by ec2.py')
            for p in ports:
                assert g.authorize('tcp', 22, 22, '0.0.0.0/0', None)

        # For every queue, start an ec2 instance.
        instances = []
        for tasks in queues.items():
            instances.extend(conn.run_instances(self._args.ami, key_name=self._args.key_name, instance_type=self._args.instance_type, security_groups=[secgroup_name]).instances)

        # As instances come up, copy over the source to them.
        instance_bookkeep = {}
        while True:
            time.sleep(10)

            statuses = collections.defaultdict(list)
            for i in instances:
                statuses[i.update()].append(i)
            print 'waiting for instances.. %s' % ', '.join(map(lambda (k, v): '%s=%s' % (k, len(v)), statuses.items()))

            for i in statuses['running']:
                if i.public_dns_name not in instance_bookkeep:

                    ssh = paramiko.SSHClient()
                    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
                    try:
                        # SCP over sources and extract.
                        ssh.connect(i.public_dns_name, username='ubuntu', key_filename='simpledist.pem')
                        scp = ssh.open_sftp()
                        scp.put(source_tarball_fn, '/home/ubuntu/source.tar.gz')
                        print "sources tarball scp'd to %s" % i.public_dns_name
                        ssh_just_run(ssh, 'cd /home/ubuntu; mkdir source; tar xzf source.tar.gz -C source')
                        print 'sources extracted'

                        # Get number of cores.
                        _, stdout, _ = ssh.exec_command('nproc')
                        cores = int(stdout.read())

                        # Add multiverse and update.
                        ssh_just_run(ssh, 'echo "deb http://us.archive.ubuntu.com/ubuntu/ precise multiverse" | sudo tee -a /etc/apt/sources.list > /dev/null')
                        ssh_just_run(ssh, 'echo "deb http://us.archive.ubuntu.com/ubuntu/ precise-updates multiverse" | sudo tee -a /etc/apt/sources.list > /dev/null')
                        ssh_just_run(ssh, 'sudo apt-get -yq update')

                        # SCP over inputs for all queued tasks and fixup tasks.
                        queue = queues[len(instance_bookkeep)]
                        for task in queue:
                            for in_name, in_fn in task.in_files.items():
                                _, stdout, _ = ssh.exec_command('mktemp')
                                new_in = stdout.read().strip()
                                print '%s -> %s' % (in_fn, new_in)
                                scp.put(in_fn, new_in)
                                task.in_files[in_name] = new_in

                        instance_bookkeep[i.public_dns_name] = {
                            'queue': queue,
                            'client': ssh,
                            'cores': cores,
                            'channels': []
                            }

                    except socket.error:
                        print '%s, sshd not up yet?' % i.public_dns_name

            if len(instance_bookkeep) == len(instances):
                break

        # Keep workers busy -- pop off a task for each free core.
        while True:
            time.sleep(1)
            print '** reap **'

            for instance_url, bookkeep in instance_bookkeep.items():

                # Reap results.
                new_channels = []
                for (ch, stdout) in bookkeep['channels']:
                    while ch.recv_ready():
                        stdout += ch.recv(1024)
                    if ch.exit_status_ready():
                        print 'task finished (%i)' % len(stdout)
                        ch.close()
                    else:
                        new_channels.append((ch, stdout))
                bookkeep['channels'] = new_channels

                # Any free cores? Start tasks.
                if len(bookkeep['channels']) < bookkeep['cores']:
                    _, stdout, _ = bookkeep['client'].exec_command('mktemp')
                    journal = stdout.read().strip()
                    cmd = 'cd source ; %s' % bookkeep['queue'].pop().to_command_line()
                    print '[%i/%i] on %s, "%s"' % (len(bookkeep['channels']) + 1, bookkeep['cores'], instance_url, cmd)
                    ch = bookkeep['client'].get_transport().open_session()
                    ch.exec_command(cmd)
                    bookkeep['channels'].append((ch, ''))


if __name__ == '__main__':
    platform.go(BasicEC2Platform)
